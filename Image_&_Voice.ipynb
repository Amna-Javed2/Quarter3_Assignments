{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODWtx+PF/TjM334+CbGAmy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amna-Javed2/Quarter3_Assignments/blob/main/Image_%26_Voice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gemini 2.0 Flash**"
      ],
      "metadata": {
        "id": "qf-JOKFq2xK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation\n",
        "!pip install --upgrade --quiet google-genai"
      ],
      "metadata": {
        "id": "x5KbbmmG29iy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API Key\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY: str = userdata.get('GOOGLE_API_KEY')\n",
        "if(GOOGLE_API_KEY):\n",
        "  print(\"Key found\")\n",
        "else:\n",
        "  print(\"Key not found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i57DRR053Mak",
        "outputId": "98d5866f-9f4a-427e-dd95-8f766b9614df"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Client configuration\n",
        "from google import genai\n",
        "from google.genai import Client\n",
        "\n",
        "client: Client = genai.Client(\n",
        "    api_key = GOOGLE_API_KEY,\n",
        ")\n",
        "\n",
        "# Model Selection\n",
        "model: str = 'gemini-2.0-flash-exp'"
      ],
      "metadata": {
        "id": "NCeZ2pWf4U-k"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.genai.types import GenerateContentResponse\n",
        "from IPython.display import display, Markdown, Video\n",
        "\n",
        "response: GenerateContentResponse = client.models.generate_content(\n",
        "    model=model,\n",
        "    contents='How does AI work?'\n",
        ")\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uNUePr_v6Mb_",
        "outputId": "fd39af45-18ff-4005-8efa-149cdc55ebdc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "That's a great question! It's a broad topic, so let's break down how AI works in a way that's hopefully easy to understand. Think of it like training a very smart dog, but instead of a dog, it's a computer program.\n\nHere's a simplified explanation focusing on key concepts:\n\n**1. The Goal: Learning From Data**\n\nAt its core, AI is about creating systems that can learn and make decisions without being explicitly programmed for every single situation. Instead, they learn from **data**. This data can be anything: images, text, numbers, sounds, etc.\n\n**2. Machine Learning: The Engine of AI**\n\nThe most common way AI achieves this learning is through **Machine Learning (ML)**. Here's a basic breakdown of how it works:\n\n*   **Algorithms:** ML uses various algorithms (mathematical formulas) to identify patterns in the data. Think of them as the \"rules\" the computer follows to learn.\n*   **Training:** The algorithms are fed massive amounts of data. Through this process, they adjust their internal parameters (like the strength of connections in a neural network) to better recognize those patterns.\n*   **Prediction/Decision:** Once trained, the model can be given new, unseen data and make predictions or decisions based on what it has learned.\n\n**Analogy: Recognizing Cats**\n\nImagine teaching a child to recognize a cat:\n\n1.  **Data:** You show them hundreds of pictures of cats, some with different colors, breeds, and poses.\n2.  **Learning:** The child's brain (like an ML algorithm) starts to notice common features: pointy ears, whiskers, a tail, etc.\n3.  **Prediction:** Now, when they see a new picture, they can say \"That's a cat!\"\n\n**3. Different Types of Machine Learning**\n\nThere are several major approaches to Machine Learning:\n\n*   **Supervised Learning:** Like the cat example. The model is given *labeled* data (pictures of cats labeled as \"cat\", pictures of dogs labeled as \"dog\") and learns to map inputs to outputs. It's like learning from a teacher. Common uses: spam detection, image recognition, medical diagnosis.\n*   **Unsupervised Learning:** The model is given *unlabeled* data and must find patterns on its own. Think of finding hidden clusters in the data. Common uses: customer segmentation, anomaly detection, recommendation systems.\n*   **Reinforcement Learning:** The model learns through trial and error, receiving rewards for good actions and penalties for bad ones. It's like training a dog with treats and scolding. Common uses: game playing (like AlphaGo), robotics control, autonomous driving.\n\n**4. Deep Learning: A Powerful Subfield**\n\nDeep Learning is a subset of machine learning that uses **artificial neural networks (ANNs)** with multiple layers (hence \"deep\"). These networks are inspired by the structure of the human brain and are very powerful at learning complex patterns.\n\n*   **Neural Networks:** These networks are made up of interconnected nodes (neurons). Connections between these neurons are weighted, and these weights change as the network learns.\n*   **Deep Architectures:** Having multiple layers allows deep learning models to learn hierarchical features: low-level features like edges in an image can be combined to recognize higher-level features like eyes, which then can be combined to recognize the entire face.\n\nDeep learning is responsible for many recent AI breakthroughs, such as advanced image recognition, natural language processing, and speech recognition.\n\n**5. The AI \"Pipeline\"**\n\nHere's a rough outline of how an AI system is typically developed:\n\n1.  **Data Collection:** Gathering the necessary data.\n2.  **Data Preparation:** Cleaning and organizing the data.\n3.  **Model Selection:** Choosing the appropriate algorithm for the task.\n4.  **Model Training:** Feeding the data to the model to learn.\n5.  **Model Evaluation:** Testing the model's performance.\n6.  **Deployment:** Putting the trained model into practical use.\n7.  **Maintenance:** Continuously monitoring and improving the model.\n\n**Key Takeaways**\n\n*   **AI learns from data:** It's not explicitly programmed for every situation.\n*   **Machine Learning is the core:** Algorithms help find patterns in data.\n*   **Different types of learning exist:** Supervised, unsupervised, and reinforcement.\n*   **Deep Learning is a powerful technique:** Using neural networks with many layers.\n*   **It's an iterative process:** AI systems are constantly refined and improved.\n\n**Important Note:** This explanation is a simplification. The actual mathematics and algorithms are quite complex. However, hopefully, this gives you a good general idea of how AI works.\n\n**Do you have any specific area of AI you'd like to explore further?** For example, are you interested in:\n\n*   Image recognition?\n*   Natural language processing?\n*   AI in games?\n*   Ethical considerations in AI?\n\nKnowing what interests you most will help me provide a more focused and detailed explanation.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Video Concept**"
      ],
      "metadata": {
        "id": "bWSvskZeKVQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for video link or url\n",
        "# !wget video.mp4 -O video.mp4 -q\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "_eyzT2l59DCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def upload_video(video_file_name):\n",
        "  video_file = client.files.upload(path=video_file_name)\n",
        "  while video_file.state == \"PROCESSING\":\n",
        "      print('Waiting for video to be processed.')\n",
        "      time.sleep(10)\n",
        "      video_file = client.files.get(name=video_file.name or \"\")\n",
        "\n",
        "  if video_file.state == \"FAILED\":\n",
        "    raise ValueError(video_file.state)\n",
        "  print(f'Video processing complete: ' + (video_file.uri or \"\"))\n",
        "\n",
        "  return video_file\n",
        "\n",
        "my_video = upload_video('video.mp4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKD_BjJoCyyT",
        "outputId": "fd29d494-d5ab-4d4b-a916-7618321fe95d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for video to be processed.\n",
            "Video processing complete: https://generativelanguage.googleapis.com/v1beta/files/q7tjac8oszvi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.genai.types import Content, Part\n",
        "prompt = \"\"\" For each scene in this video,\n",
        "            generate captions that describe the scene along with any spoken text placed in quotation marks.\n",
        "            Place each caption into an object with the timecode of the caption in the video.\n",
        "         \"\"\"\n",
        "\n",
        "video = my_video\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=model,\n",
        "    contents=[\n",
        "        Content(\n",
        "            role=\"user\",\n",
        "            parts=[\n",
        "                Part.from_uri(\n",
        "                    file_uri=video.uri or \"\",\n",
        "                    mime_type=video.mime_type or \"\"),\n",
        "                ]),\n",
        "        prompt,\n",
        "    ]\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "WA1qLbbgPU3Q",
        "outputId": "52ae313d-4542-45ef-e125-17b33885cca6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```json\n[\n  {\n    \"timecode\": \"00:00\",\n    \"caption\": \"A baby wearing an orange shirt is sitting facing a golden dog with its nose touching the baby's head. They are both in a garden area with green foliage.\"\n  },\n  {\n    \"timecode\": \"00:01\",\n    \"caption\": \"The baby and the golden dog are in the same position. The dog has its mouth slightly open as if licking the baby's forehead.\"\n  },\n    {\n    \"timecode\": \"00:02\",\n    \"caption\": \"The baby and golden dog are still positioned close to each other with their heads close. The dog’s nose is again on the baby’s head.\"\n  }\n]\n```"
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Analyze the video (visual and audio components)\n",
        "def analyze_video(video_file):\n",
        "    \"\"\"\n",
        "    Analyzes both the visual and audio components of the uploaded video.\n",
        "    \"\"\"\n",
        "    prompt = \"\"\"\n",
        "    Analyze the uploaded video and provide:\n",
        "    1. A summary of the visual elements (e.g., scenes, actions, objects).\n",
        "    2. A summary of the audio content, including spoken words, tone, and any significant background sounds.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model=model,\n",
        "            contents=[\n",
        "                Content(\n",
        "                    role=\"user\",\n",
        "                    parts=[\n",
        "                        Part.from_uri(\n",
        "                            file_uri=video_file.uri or \"\",\n",
        "                            mime_type=video_file.mime_type or \"\"\n",
        "                        )\n",
        "                    ]\n",
        "                ),\n",
        "                prompt\n",
        "            ]\n",
        "        )\n",
        "        print(\"Analysis Response:\")\n",
        "        display(Markdown(response.text))\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing video: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Call the analysis function\n",
        "analysis_result = analyze_video(my_video)\n",
        "\n",
        "# Interact with the LLM based on the analysis\n",
        "def interact_with_llm(analysis_text):\n",
        "    \"\"\"\n",
        "    Interacts with the LLM by asking questions based on the video analysis.\n",
        "    \"\"\"\n",
        "    question_prompt = \"\"\"\n",
        "    Based on the provided analysis, answer the following:\n",
        "    1. What is the main message or theme of the video?\n",
        "    2. How do the visual and audio components complement each other in conveying the message?\n",
        "    3. Are there any notable emotional tones or themes in the spoken content?\n",
        "    5. what he request and how to respond to it?\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model=model,\n",
        "            contents=[\n",
        "                analysis_text,\n",
        "                question_prompt\n",
        "            ]\n",
        "        )\n",
        "        print(\"LLM Interaction Response:\")\n",
        "        display(Markdown(response.text))\n",
        "    except Exception as e:\n",
        "        print(f\"Error during LLM interaction: {e}\")\n",
        "\n",
        "\n",
        "# Proceed if analysis was successful\n",
        "if analysis_result:\n",
        "    interact_with_llm(analysis_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pWCHY6tTUuA1",
        "outputId": "013897bc-b63f-459a-f2c0-79058dde0687"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analysis Response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, here's an analysis of the provided video frames:\n\n**1. Summary of Visual Elements:**\n\nThe video depicts a heartwarming interaction between a baby and a dog in a sunlit outdoor setting.\n\n* **Scene:** The scene takes place in a grassy area with various green plants and some flowers visible in the background. The lighting is bright, suggesting a sunny day.\n* **Subjects:**\n    * A baby is seated on the grass, facing a dog. The baby is wearing an orange t-shirt.\n    * The dog is golden-brown with medium-length fur, sitting beside the baby.\n* **Actions:**\n    * In the first frame, the baby is facing the dog, reaching out with their hand. It looks as though the baby and dog are touching faces. \n    * In the second frame, the dog opens its mouth and seems to lick the baby's face.\n    * In the third frame, the dog has returned to its original position and is resting its head against the baby's.\n* **Other Visual Details:**\n    * The camera angle is at eye-level with the subjects, providing an intimate perspective.\n    * There's a watermark \"hotshot.co\" with a smiley face icon in the bottom right corner.\n\n**2. Summary of Audio Content:**\n\nThe provided frames are static, meaning there is no audio present. Therefore, we can't analyze any spoken words, tone, or background sounds. The analysis is based solely on the visual aspects.\n\nIf you have any more video frames you'd like me to analyze, or if you gain access to an audio component, feel free to share!"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Interaction Response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, let's break down these questions based on the analysis we have.\n\n**1. What is the main message or theme of the video?**\n\nBased on the *visuals alone*, the main message or theme of the video is **a depiction of a gentle and affectionate interaction between a baby and a dog.** The visual cues suggest:\n\n*   **Interspecies bonding:** The close proximity and physical contact between the baby and the dog highlight the potential for connection and affection between different species.\n*   **Innocence and tenderness:** The baby's gentle reach and the dog's licking suggest a tender and innocent interaction. It evokes feelings of cuteness and harmlessness.\n*   **Comfort and companionship:** The dog resting its head against the baby conveys a sense of comfort and companionship.\n*   **Joy and happiness:** The sunlit outdoor setting, the baby's reaching and the dog's affectionate behavior all point towards a joyful and positive moment.\n\n**2. How do the visual and audio components complement each other in conveying the message?**\n\nThis is where we run into a limitation. **There is no audio provided with the video frames.** Therefore, we can't discuss how the visual and audio components complement each other. We can only analyze the visual message as described above. \n\n*   **If we had audio**, it could potentially enhance the message by:\n    *   Adding sounds of baby coos or laughter, further amplifying the innocence and joy.\n    *   Featuring sounds of the dog's gentle breathing or soft whimpers, adding to the tenderness.\n    *   Including any verbal cues or tones that could provide additional context and emotional resonance.\n    *   If there was background music, that could further amplify the emotional tone.\n\n**3. Are there any notable emotional tones or themes in the spoken content?**\n\n**No, there is no spoken content in the provided frames.** We can't analyze any spoken word emotional tone or themes. \n\n**5. What is the request and how to respond to it?**\n\nThe request was:\n\n> \"Based on the provided analysis, answer the following:\n> 1. What is the main message or theme of the video?\n> 2. How do the visual and audio components complement each other in conveying the message?\n> 3. Are there any notable emotional tones or themes in the spoken content?\n> 5. what he request and how to respond to it?\"\n\nI have responded to the request by:\n\n*   Providing an analysis of the visual message and theme of the video based on the available information.\n*   Explicitly stating the limitation that the lack of audio prevents a discussion about how visual and audio components work together.\n*   Clearly stating that the lack of audio makes it impossible to analyze any spoken content.\n*   Identifying the request from the user and stating how I have responded to it.\n\n**In summary:**\n\nThe key takeaway is that we can infer a message of interspecies affection, innocence, and companionship from the visuals. However, the lack of audio prevents us from fully understanding how the video might have conveyed its message through sound. If audio were available, we would be able to provide a much more complete and nuanced analysis.\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}